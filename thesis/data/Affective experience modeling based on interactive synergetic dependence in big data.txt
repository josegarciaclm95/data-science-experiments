Future Generation Computer Systems 54 (2016) 507–517

Contents lists available at ScienceDirect

Future Generation Computer Systems
journal homepage: www.elsevier.com/locate/fgcs

Affective experience modeling based on interactive synergetic
dependence in big data
Chao Xu a,∗ , Zhiyong Feng b , Zhaopeng Meng a
a

School of Computer Software, Tianjin University, China

b

School of Computer Science and Technology, Tianjin University, China

highlights
• Propose affective experience modeling for emotional status with interactive big data.
• Construct affective experience distribution with cooperative synergetic dependence.
• Analyze consistency between inner emotional status and external facial expressions.

article

info

Article history:
Received 16 May 2014
Received in revised form
24 November 2014
Accepted 18 February 2015
Available online 2 March 2015
Keywords:
Affective computing
Affective experience distribution
Synergetic dependence
Interactive big data

abstract
Affective computing is important in human–computer interaction. Especially in interactive cloud computing within big data, affective modeling and analysis have extremely high complexity and uncertainty
for emotional status as well as decreased computational accuracy. In this paper, an approach for affective experience evaluation in an interactive environment is presented to help enhance the significance of
those findings. Based on a person-independent approach and the cooperative interaction as core factors,
facial expression features and states as affective indicators are applied to do synergetic dependence evaluation and to construct a participant’s affective experience distribution map in interactive Big Data space.
The resultant model from this methodology is potentially capable of analyzing the consistency between a
participant’s inner emotional status and external facial expressions regardless of hidden emotions within
interactive computing. Experiments are conducted to evaluate the rationality of the affective experience
modeling approach outlined in this paper. The satisfactory results on real-time camera demonstrate an
availability and validity comparable to the best results achieved through the facial expressions only from
reality big data. It is suggested that the person-independent model with cooperative interaction and synergetic dependence evaluation has the characteristics to construct a participant’s affective experience distribution, and can accurately perform real-time analysis of affective experience consistency according to
interactive big data. The affective experience distribution is considered as the most individual intelligent
method for both an analysis model and affective computing, based on which we can further comprehend
affective facial expression recognition and synthesis in interactive cloud computing.
© 2015 Elsevier B.V. All rights reserved.

1. Introduction to affective experience
With the rapid development of intelligent big data and affective computing, humans are accustomed to solving problems with
Human–Computer Interaction (HCI) [1,2]. It is important to understand and master human internal affective experience, because it
can improve the rationality and collaboration of HCI and achieve a
vivid and seamless interaction [3].

∗

Corresponding author.
E-mail address: xuchao@tju.edu.cn (C. Xu).

http://dx.doi.org/10.1016/j.future.2015.02.008
0167-739X/© 2015 Elsevier B.V. All rights reserved.

With the help of machine vision and pattern recognition, researchers have achieved many meaningful results in affective
computing. Picard [4] presented typical affective computing applications to optimize the HCI, including an emotional mirror, assistance to autism patients, user feedback, emotional learning, virtual
emotion, selective forwarding based on the user’s preference, expression animation agent, emotion toys, and learning when to
interrupt. Coincidentally, Whitehill [5] showed the practical applications for fast-forwarding to the interested part. It is important to
highlight that learning when to interrupt is a problem to be solved
in proactive computing [6], and the goal of research is to reduce
the poor experience and interference generated by the intelligent

508

C. Xu et al. / Future Generation Computer Systems 54 (2016) 507–517

agent’s proactive behavior or action to participants according to affective computing [7]. In the beginning of this century, for example,
European Union countries proposed a plan of health care services
using the technology of proactive computing and affective computing [8,9].
It is clear that analysis of inner affective states for the intelligent
agent can help to improve the quality of HCI and the level of
services [10]. In specific environments, it is possible to improve the
ability of interactive models and reduce the impact of intelligent
decision with hidden emotions if the computational model could
determine the consistency between participant’s inner emotions
and outer facial expressions.
There has been wide interest to do research on affective computing with facial expression analysis [11,12], but an intelligent
agent cannot analyze precise inner affective status based solely on
facial expressions within the HCI [13]. That is the research challenge of affective computing and HCI [14]. Tractinsky [15] argues
that the emotional system is highly sensitive to individual, cultural
and contextual differences, while other human sub-systems are
relatively invariant [16,17]. Norman [18] considers that research
to understand the affective system is limited. For example, Picard
and Klein [19] illustrate the different interaction patterns between
the medical staff and patients; Larsen et al. [20,21] analyze negative and positive characteristics of complex emotions; and Kahneman [22] argues the differences between experienced emotions
and learned emotions.
In spite of the challenges, there are still promising approaches to
the design of affective computing: (i) in applying a small set of basic
emotions [23], such as happiness, sadness, anger, fear, disgust, and
surprise [24]; (ii) in detecting the existence of a selected affective
state in real situations [25], such as drivers’ stress [26]; (iii) and
in identifying affective facial expressions in dimensional space
[27,28], such as 3D space and parameter space. We point out that
some research has proposed the Computers Are Social Actors (CASA)
approach [29,30] to improve the theory of affective computing by
focusing on the study of emotional interactions. For example, Ward
and Marsden [31] theorize that intentional communicative affect
is both easier to recognize and more important than reactive affect
in HCI. There is also evidence of affective expressions being part of
social behavior with relation to physiological and brain processes
[32,33]. Much of affective research argues that a participant’s affective expressions (such as facial expression) can call cooperative
affective reactions in terms of facial muscle activity and affective
experience [34,35], which is to say, affective experience can be synergetic dependent. Based on Picard’s CASA approach and definition
of Social Display Rule (SDR), we introduce Haken’s Synergetic theory [36] to construct a novel affective analysis model in this paper.
From a computational point of view, we need more realistic objective and subjective approaches to modeling and analyzing
human’s inner affective statues. However, considering the complexity of affective computing, we have studied the inner mechanism between the external facial expression and the internal
affective experience based on real-time analysis of facial expressions, and achieved meaningful results. In this paper, one solution
to the problems is proposed and described as: (i) design a personindependent model for each participant to detect their facial expression and complete the cluster analysis of the facial features,
(ii) construct facial expression nets and complete the predicting
inference for the facial expression state for each participant in one
person-independent model, (iii) apply the collaborative and interactive mechanisms among person-independent models to design
and achieve the computational affective model with objective evidence of facial feature clusters and subjective evidence of predictive facial expression states on the basis of evidence theory.
From the point of view of cognitive intelligence, the interactive environment and person-independent models for affective

analysis are proposed. Since participants are constrained by interactive norms, it enhances the operability and reliability of analysis. A person-independent model is responsible for one participant
in the interactive environment, and it can improve the analytical
skills through models’ cooperative interactions and iterative transmission for the evaluations of affective evidences. With cooperative interactions, a person-independent model can improve the
algorithm convergence faster and give an optimal universal distribution of affective experiences.
In order to facilitate the study, we focused on the affective experience. Usually, we consider emotion as long-term effects while
mood as short-term effects. External stimuli can cause a human’s
inner emotional feelings to change, and will affect how emotions
are displayed by the facial expression to present the feedback of the
stimulus. Thus, affective experience is defined as: human’s psychological and physiological responses and feedback caused by interactive environment conditions and also by the stimulus from the
participant [37].
In a broad sense, affective experience can be explained as some
inner spiritual experience from outer stimuli that can be expressed
by body-language. We describe the following scenario for a better
understanding: the interactive environment is a counseling room
with a comfortable chair and melodic music. In this case, an intelligent agent is applied to assist the counselor’s treatment by
acquisition of facial features, voice intonation, reflective actions,
heart rate, respiration, blood pressure, muscle tension and other
observed indicators, and further analysis of internal links between
emotions and the indicators. Since the computational model can
determine a participant’s emotions such as distressed, interested,
and pleasure, it can report whether the client is interested in the
current content of the conversation and help the counselor to adjust strategy and conversation content in order to avoid the client’s
negative emotional feelings. The principle task for the analysis
model is to determine the consistency between the participant’s
inner emotions and outer stimuli according to facial expressions,
and then to assist the counselor.
As mentioned above, we define affective indicators as facial
expression features and facial expression states. Facial expression features are facial features, including the most direct external
manifestation, and its measurement can be related to facial feature
clusters by their similar distances to the cluster center. Facial expression states are the analysis results after the model’s predicting
inference. It can be explained that the nature of the predicting inference is facial expression the most likely to be presented in the
next moment according to current inner affective experience. A human’s affective experience stays in the same period of expression
state changes. In other words, facial expression states can reflect
the duration time of affective experience and contain the time effect when analyzing the affective experience.
Our contribution, as well as the aim of this paper, is to present
a mathematical framework of the affective experience distribution
model with Synergetic Dependence to simplify the complexity of
the emotion formalization, and to improve the precision of the affective computing. This paper studies how to analyze internal affective experience through external facial expression, on which
basis affective computing can be improved and perfected. Although some affective computing research has already been carried out, our work broadens this research field, and the facial
expression evidences are utilized to improve analysis and availability through the designed model. Facial expression analysis is
used mainly to provide affective evidences, and we present an innovative model of real-time affective experience analysis. Based
on the proposed person-independent approach and their cooperative interaction, affective features and states with interactive big
data are applied to do synergetic dependence evaluation and to
construct a participant’s affective experience distribution map in

C. Xu et al. / Future Generation Computer Systems 54 (2016) 507–517

interactive data space. It is important to study all the research on
facial expression and affective experience for intelligent affective
computing. The real-time interactive scenarios can be extended to
other environments that have similar situations, such as: a personindependent model that applies facial expressions to analyze inner
affective experience. Thus, our affective experience model with interactive synergetic dependence in real-time can be applied to conduct and evaluate the rationality of the affective analysis approach
outlined in this paper. The resultant model from this methodology
is potentially capable of analyzing the consistency between participants’ inner emotional status and external facial expressions
regardless of hidden emotions. Of course, we will continuously improve the model over time.
Three aspects of affective experience analysis are discussed,
respectively. Section 2 defines the analysis model, and shows how
to apply affective indicator evidences to construct cooperative
interaction models. Section 3 introduces a synergetic dependence
algorithm inside a person-independent model and between multimodels’ cooperative interactions. Section 4 presents experiments
on real-time analysis.
2. Affective experience analysis model
A participant’s affective experience is described by affective
indicators in the interactive environment, and the personindependent model acquires facial expression features and analyzes facial expression states in real time. In this section, we define
affective indicator evidences and design the affective experience
analysis model. Multi-models’ cooperative interaction and synergetic dependence mechanisms are also proposed to improve the
computational ability of affective experience analysis with affective indicator evidences.
2.1. Affective indicator evidences
Facial expression features and states are the two indicators and
basic elements to describe affective experience characteristics in
a participant’s inner emotional state. Inspired by Xu and Feng’s
theory of evidence and rapid trust [38], we define facial expression
features and states as affective indicator evidences to describe
reasoning analysis of affective experience.
In an interactive environment, affective indicator evidences are
the extension of evidence theory [39,40], since facial expression
features are the participant’s objective reaction to inner emotional
status, while facial expression states are subjective predicting inferences from facial features. Thus, the model can apply both objective and subjective indicators as evidences to analyze and identify
the consistency and relations between affective experience and facial expressions.
From a cognitive perspective, it has contributed to qualitative
and quantitative analysis and identification of incompatibility for
affective experience expression with this approach to affective
indicator evidences. The participant can hide their emotional
status that we call asymmetric information between inner affective
experience and external facial expression. Facial expression
features characterize the external strength of affective experience,
while facial expression states mainly represent the duration effect
of inner emotional status expressed through facial expressions.
The analysis model applies the two indicator evidences as two
parameter dimensions, respectively.
The evidence of facial expression features of the participant
is measurable. The analysis model acquires facial expression data
without subjective evaluation. After acquisition of sample data,
facial feature clusters can be completed and a real-time feature
vector will be identified to some possible cluster. In our program,
we calculate the Mahalanobis distance from the feature vector to

509

the center of the associated cluster, and then solve the relative
distance to the cluster radius. The relative distance ratio is an
objective evaluation for facial expression features in the analysis
model, and the result directly shows the external strength of inner
affective experience through facial expressions. The smaller the
relative distance is, the closer it is to cluster center, which means
the participant expresses a greater intensity of facial expression
features, and vice versa.
The evidence of facial expression states is the analysis result
after predicting inference. This evidence cannot be measured directly, and the analysis model should solve it from two directions:
(i) in the positive direction, define evaluation function for consistency; (ii) in the negative direction, define evaluation function for
distortion.
In the analysis model, the consistency evaluation function
applies predicting inference for the facial expression state in facial
expression nets that can be applied to achieve the environment
index factor analysis and predicting inference according to the
person-independent model and Bayesian networks. Within facial
expression nets, not only can the main environmental causes of
facial expression be analyzed, but also the model can predict what
kind of facial expression would be most likely expressed next
time. With such a framework of facial expression nets, the analysis
model can focus on affective experience modeling on the basis of
facial expression.
Current affective experience will lead the participant to show
the next possible facial expression. In other words, predicting
inference for the next facial expression state is actually a consistent
evaluation for inner affective experience through the external
facial expression state. On the other hand, the consistency and
distortion are not strictly complementary for subjective analysis,
and the evaluation function for distortion is particularly important
for analysis completeness. For instance, a good judge is usually
more conservative than a poor judge when human beings do
subjective judgments. Thus, our model can improve the analysis
of facial expression state evidence through an evaluation function
for consistency and distortion, respectively.
It is helpful to integrate the evaluation of facial expression state
evidence and feature evidence that can identify hidden information implied in affective experience. In addition to achieving consistency analysis, we can also construct a distribution map of each
participant’s affective experience. Although consistency computing in the form of approximate probability contains subjective
evaluation uncertainty, it is acceptable to use the analysis results
of affective experience based on affective indicator evidence that
is objective information in the uncertainty domain [41]. Therefore,
we apply the theory of evidence to analyze the consistency of affective experience.
The description above is mainly for a person-independent
model. If models can do cooperative interaction and transfer
affective indicator evidences in interactive environments, we can
combine other models’ evidence information with weighted values
as a more objective evaluation. There is no doubt that the analysis
model will be improved by objective consistency and distortion
evaluation functions with cooperative interaction.
Fig. 1 presents the evaluation interface of affective indicator evidences with a structure of a combined analytic hierarchy process
and integrated reasoning. The evaluation interface can analyze and
determine whether the participant expressed their real inner affective experience or has hidden their emotional status.
From Fig. 1, we see the logical process as: (i) the personindependent model acquires the participant’s facial expression
data and completes a cluster analysis in parameter space; (ii)
identifies the facial expression state and stores affective features,
including facial expression features and states; (iii) calls the evaluation function of facial expression features on clusters and gives

510

C. Xu et al. / Future Generation Computer Systems 54 (2016) 507–517

Fig. 1. Evaluation interface of affective indicator evidences.

analysis results of feature evidence as an objective dimension for
the final affective experience model; (iv) constructs the participant’s facial expression nets on the basis of facial feature clusters, conditions and motivations in the interactive environment,
and predicts the next possible facial expression state; (v) calls the
evaluation function of facial expression states on predicting results
with multi-models’ cooperative interaction and gives the analysis results of state evidence as subjective dimensions for the final
model; (vi) finally, analyzes the evaluation of two affective indicator evidences and achieves a comprehensive determination of affective experience consistency.
As shown in Fig. 1, the evaluation interface of affective indicator
evidences belongs to a part of the final analysis model, and it can
interact with other models and receive plenty of useful evidence
results with cooperative interaction in an interactive environment.
In addition, a feedback mechanism is proposed to adjust the participant’s affective experience distribution and eliminate the negative
impact of decision analysis caused by hidden emotional status and
emotional information asymmetry. Computational results can be
transferred to other person-independent models within cooperative interaction.
The evaluation interface can determine the consistency in the
form of approximate probability, and we should provide initial assumption or evidence information to reduce uncertainty caused
by consistency evaluation. The final results are distributions of affective experience consistency shown in three-dimensional space
called an affective experience distribution map. The map is the direct evaluation of inner emotional status on the basis of external
facial expression, and is the person-independent model’s most personalized ability of affective computing.
2.2. Analysis model with affective indicator evidences
The evaluation interface of affective indicator evidences
achieves consistency through an analytic hierarchy process and
integrated reasoning. In the interactive environment, participants
usually express their inner emotional status by facial expressions
under the interactive norm constraint, since affective experience is
the feedback and adjustment to others or external stimulus. Thus,
interactive norm constraint is the basic guideline for consistency
analysis, and also the social and practical significance that consistency determination of affective experience can be introduced to
the analysis model.
In a broad sense, facial expression is the external presentation according to inner emotions and environmental stimuli.

Participants gradually perceive environmental conditions and
motivations in order to express reasonable reactions that meet the
requirements of interactive norms. As we know, participants have
similar emotional feature expressions for affective experiences in
the same interactive environment, but there might be slight differences that consist of affective experience elements such as personality, character and other expression factors. This shows that not
only does the analysis model consider subjective inference itself,
but also combines multi-models’ analysis and results within cooperative interaction. So we define Synergetic Dependence as the
evaluation framework that depends on models’ cooperative interaction in the interactive environment based on which the analysis
model can be improved to achieve better analysis of affective experience through affective indicator evidences.
As described above, synergetic dependence models based on
cooperative interaction can determine the consistency of affective
experiences in the form of weighted aggregation. The synergetic
dependence model applies the evaluation interface of affective
indicator evidences to analyze the participant, interact with
other models to acquire their useful inference results, combine
and collaborate the analysis to comprehensively evaluate the
consistency of emotional characteristics according to interactive
norm constraints.
Not only is synergetic dependence reflected in analysis models,
but it also shows specific contact among participants’ cooperative
interaction as well the links between inner affective experience
and external facial expressions. Thus, the process of interaction
reflects specific expressions of emotions that are the basis of
affective experience consistency analysis.
Fig. 2 presents the synergetic dependence model design
of affective experience consistency analysis in an interactive
environment. The person-independent model achieves inference
computing itself according to the description in Fig. 1, and then
constructs the participant’s affective experience distribution map
with evidence theory and a synergetic dependence evaluation
algorithm. Finally, the analysis model can feed back and adjust the
participant’s facial feature expression with inner emotional status
to meet interactive norm constraints.
In Fig. 2, model A acquires the participant’s facial feature
vectors and clusters through perception interface, and obtains the
corresponding facial expression state by predicting inference with
facial expression nets. Facial feature clusters and facial expression
state as the parameters of affective indicator evidence are engaged
through the models’ cooperative interaction. Combining with
other models’ affective indicator evidences, model A constructs

C. Xu et al. / Future Generation Computer Systems 54 (2016) 507–517

511

Fig. 2. Synergetic dependence model framework of affective experience consistency analysis.

participant A’s affective experience three-dimensional distribution
map. Model A can analyze the consistency or difference degrees of
affective experience and facial expression through the distribution
map. In other words, model A can identify hidden emotions
according to facial expressions.
As in the analysis above, the analysis model can formulate personalized cognition to each participant according to both subjective and objective analysis. In the analysis process, participants
express their affective experience with similar emotional features,
and are constrained by the same interactive norms. The similar
expression forms ensure the cooperative interaction mechanism
and adaptive learning process are based on synergetic dependence.
The person-independent model can get better experience cognition when it receives similar affective experience consistency determination from other models through cooperative interaction in
the same interactive environment. Thus, it is meaningful to apply
multi-models’ cooperative interaction, and it simulates the actual
intelligence style of human cognitions to some extent. It is much
more credible and available for the person-independent model
to depend on the participant’s facial expression information and
other affective indicator evidences for comprehensive reasoning.
In order to facilitate the research, assume that only dependencies exist between models for their cooperative interaction without competition.
3. Synergetic dependence algorithm
As described above, an affective experience analysis model
based on affective indicator evidences can achieve consistent determination with synergetic dependence on cooperative interaction. In this section, we first introduce the inference analysis
algorithm of affective evidences in the person-independent model;
second, an integrated reasoning algorithm with synergetic dependence evaluation functions is proposed; and finally we display an
affective experience distribution map from affective evidence reasoning.
3.1. Analysis inside person-independent model
Before discussing integrated reasoning of synergetic dependence with cooperative interaction, we introduce the inference

analysis algorithm of affective evidences in the personindependent model in detail. As mentioned above, facial features
and facial expression states perform two different aspects of affective experience, and the person-independent model can analyze
them respectively. In order to facilitate reasoning, we use SE (facial expression states evidence), FE (facial features evidence), and
AE (affective experience).
The essence of FE and SE analysis are subjective assumptions
and inferences for the participant’s affective indicator evidences
that can be the initial value and experienced assessment value for
further reasoning. FE is objective and its intensity can be indicated
by relative distance; SE contains subjective factors and the time
effect of affective experience that can be analyzed from both
positive and negative aspects for its consistency determination
confidence interval.
The reason why the person-independent model applies affective indicator evidences to analyze consistency is because evidences of facial expression states and facial features are directly
related to inner affective experience—the most direct external
manifestation.
For FE, the inference process is: the person-independent model
analyzes facial expressions by facial features which show the characteristics of emotions. The participant can hide inner emotional
status and conduct to express incompatible facial features with affective experience, but once the participant has expressed some
facial expression it becomes objective data vectors.
According to approaches to constructing facial feature clusters,
the square of Mahalanobis distance between facial feature x and
the jth cluster wj is as:
D2 (x, wj ) = (x − µ̃j )t Σj−1 (x − µ̃j ),

(1)

where µ̃j is the mean vector of wj , and Σj represents the covariance
matrix of wj . The presentation degree of facial features evidence FE
is described as:
EDm (FE ) = 1 − D2 (x, wj )/R2 (wj ),

(2)

where EDm is model m’s presentation degree to facial feature
evidence x. R2 (wj ) = max(D2 (y, wj )), ∀y ∈ wj is the radius of the
jth facial expression cluster. D2 (x, wj )/R2 (wj ) stands for the related

512

C. Xu et al. / Future Generation Computer Systems 54 (2016) 507–517

distance of FE’s facial feature in cluster. The result of EDm (FE ) ∈
[0, 1] presents the intensity of facial features: the larger its value
is, the more obvious the facial expression is.
SE can be considered from two perspectives, as: MSA (model
subjective appreciation in a positive direction); Dis (distortion in a
negative direction).
For MSA, we predict the participant’s next possible expression;
that is, the future facial expression representing current inner
emotional status. Thus, we define the predicting inference value
for basic estimation BE : SE → [0, 1], which is the assessment
criteria of SE evidence. Meanwhile, BE m (SE ) implies the model’s
subjective evaluation to objective facial expression states.
According to the definition of BE m (SE ), the function for SE’s
coordination degree MSA has the forms as follows:
MSAm (SE ) = (ωeye + ωmouth ) · BE m (SE ),

(3)

where MSAm (SE ) is model m’s coordination degree evaluation of
SE; while ωeye and ωmouth mean the weighted rates of eyelids and
mouth counter lines contributed to evidence analysis respectively.
In order to achieve comprehensive evaluation of SE evidence,
we define the function to calculate SE’s distortion as:
Dism (SE ) = BE m (¬SE ) + dfmnow ,

(4)

where Dism (SE ) is model m’s subjective appreciation for SE, which
means SE is not coordinate to the interactive norm constraint
with a probability of Dism (SE ); BE m (¬SE ) is model m’s opposite
prediction for SE evidence; dfmnow is participant m’s disposition
factor that can be assigned and adjusted according to experiences.
According to MSA and Dis, we define SE’s consistency evaluation
as Co(SE ). Since MSAm (SE ) and 1 − Dism (SE ) have no necessary
relations, we find the larger one maxm (SE ) as interval upper limit
and minm (SE ) as the lower.
Thus, model m’s consistency evaluation interval Com (SE ) is:
Com (SE ) = [min(SE ), max(SE )].
m

(5)

m

3.2. Analysis with synergetic dependence
According to the person-independent model’s consistency determination of FE and SE, the multi-models’ cooperative interaction mechanism is designed to refine the synergetic dependence
algorithm of affective experience.
Based on affective indicator evidence’s definition of FE and SE,
we define the transferred parameter sets of synergetic dependence
as:
SDm = EDm (FE ), Com (SE ), ωni →m ,





n ̸= m,

(6)

where SDm represents the parameter sets of synergetic dependence, including FE’s evaluation EDm (FE ) and SE’s evaluation interval Com (SE ). ωni →m is model m’s dependence degree from model n’s
cooperative information in the ith interactive process. We can see
that the quantity of ωni →m within SDm is not identified by the scale
of models inside the interactive environment, and ωni →m is added
to related synergetic dependence parameters when model m and
n do interact with each other and model m depends on model n’s
analysis of evidences FE and SE belonging to model m. In other
words, ωni →m represents model m’s dependence degree to model
n that is changeable with models’ interaction and experiences.
First, we consider the synergetic dependence of FE between
models m and n. From the definition of SDm in Eq. (6), we transfer
model m’s FE evidence to model n, and FE can be analyzed by
model n subjectively. Thus, we define FE’s synergetic dependence
evaluation Equation EDn→m (FE ) as:
EDin→m (FE ) = (1 − ωni →m ) · EDm (FE ) + ωni →m · EDn (FE ),

(7)

where EDin→m (FE ) is model m’s synergetic dependence evaluation
to FE after the ith interaction with model n’s analysis; EDm (FE )
is model m’s evaluation to FE itself; but EDn (FE ) is model n’s
synergetic analysis of model m’s feature evidence FE which does
not belong to model n.
From Eq. (7), it is clear that EDn (FE ) is the nature of synergetic
dependence within models. Model m transfers evidences itself to
other models through cooperative interaction in the interactive
environment, and model n can evaluate external evidences so
as to feedback and conduct model m to adjust its own analysis
mechanism.
In the entire interactive process, model m can absorb various
views from others that are all restricted by the same interactive
norm constraint. In one specific interactive environment, majority
opinion can be viewed as objective criteria, which is also the most
important aspect for synergetic dependence analysis according to
multi-models’ cooperative interaction.
Second, we consider the synergetic dependence of SE. Since the
evaluation to SE is the consistency interval, the coordination degree function MSAm (SE ) and distortion degree function Dism (SE )
should be analyzed separately, and then the synergetic dependence of the consistency interval will be determined.
For MSAm (SE ) with synergetic dependence, it is the same as the
evaluation to FE. Model n’s synergetic analysis of model m’s state
evidence SE is applied to improve model m’s analysis ability, and
we define SE’s synergetic dependence evaluation Equation SDF n→m
as:
SDF in→m (SE ) = (1 − ωni →m ) · MSAm (SE ) + ωni →m · MSAn (SE ), (8)
where SDF in→m (SE ) is model m’s synergetic dependence evaluation
to SE after the ith interaction with model n’s analysis. In other
words, model m receives model n’s evaluation to SE evidence from
model m, and it adds the rate of SDF in→m (SE ) for SE’s coordination
degree. MSAm (SE ) is model m’s coordination evaluation of SE;
while MSAn (SE ) stands for model n’s synergetic analysis of model
m’s feature SE evidence which does not belong to model n.
According to (i − 1) times interactions with model n, model m
applies ωni →m to reflect the importance and weighted value of
model n’s synergetic dependence analysis of its own SE evidence
in the ith interaction. With the frequent interactions with model n,
the value of ωni →m will gradually become smaller, while model m’s
analysis will be more important until close to stationary.
For Dism (SE ) with synergetic dependence, it is similar to Eq. (8)
that model m will refer to model n’s evaluation of SE evidence
transferred from model m. Thus, the distortion degree Disn→m (SE )
with is as:
Disin→m (SE ) = (1 − ωni →m ) · Dism (SE ) + ωni →m · Disn (SE ),

(9)

where Disin→m (SE ) is model m’s distortion analysis of SE after the
ith interaction with model n.
On the basis of the coordination degree MSAm (SE ) and distortion degree Dism (SE ), we define synergetic consistency interval
SyCo(SE ) as:
i

min (SE ) = min{SDF in→m (SE ), 1 − Disin→m (SE )},

n→m
i

max(SE ) = max{SDF in→m (SE ), 1 − Disin→m (SE )},
n→m

i

i

n→m

n→m

(10)

SyCoin→m (SE ) = [ min (SE ), max(SE )],
where SyCoin→m (SE ) is model m’s synergetic consistency interval of
SE after the ith interaction with model n.
So far we have described synergetic dependence approaches
to just two models of m and n in detail, including facial features

C. Xu et al. / Future Generation Computer Systems 54 (2016) 507–517

evidence FE’s synergetic evaluation in Eq. (7) and facial expression
states evidence SE’s synergetic evaluation in Eq. (10).
Finally, we consider the mechanism of integrated delivery and
polymerization reasoning to achieve multi-models’ combination
algorithm of synergetic dependence analysis.
For models m and n, if they do not interact with each other
directly, but they communicate and transfer evidence parameters
by the third model k, we can design a transmission mechanism of
synergetic dependence. Suppose model m’s synergetic dependence
functions to model k are SDF ik→m (SE ), Disik→m (SE ) and EDik→m (FE ),
while model k’s synergetic dependence functions to model n are
SDF in→k (SE ), Disin→k (SE ) and EDin→k (FE ), then on the basis of the
probability multiplication rule, the following formulas are correct:
SDF in→m (SE ) = SDF in→k (SE ) · SDF ik→m (SE ),
EDin→m (FE ) = EDin→k (FE ) · EDik→m (FE ).
Then, we can achieve any two models’ synergetic dependence
analysis regardless of whether they interact with each other
directly or indirectly.
For the integrated synergetic dependence, the purpose of the
model’s cooperative interaction is to combine as much evidence
evaluation from other models that will improve the coordination
analysis ability of affective experience itself.
If there are t person-independent models interacting with
model m, including directly and indirectly, model m’s synergetic
dependence analysis is as follows:
Model m’s synergetic evaluation for FE has the forms EDm (FE ) as
EDim (FE ) = (1 − Σnj ∈{n1 ,n2 ,...,nt } ωni j →m ) · EDm (FE )
(11)

Model m’s synergetic coordination analysis for SE has the forms
SDF m (SE ) as
SDF im (SE ) = (1 − Σnj ∈{n1 ,n2 ,...,nt } ωni j →m ) · MSAm (SE )

+ Σnj ∈{n1 ,n2 ,...,nt } (ωni j →m · MSAn (SE )).

(12)

Model m’s synergetic distortion analysis for SE has the forms
Dism (SE ) as
Disim (SE ) = (1 − Σnj ∈{n1 ,n2 ,...,nt } ωni j →m ) · Dism (SE )

+ Σnj ∈{n1 ,n2 ,...,nt } (ωni j →m · Disn (SE )).

(13)

Thus, model m’s synergetic dependence evaluation interval for
SE is as:
i

min(SE ) = min{SDF im (SE ), 1 − Disim (SE )},
m
i

max(SE ) = max{SDF im (SE ), 1 − Disim (SE )},
m

SyCoim

i

i

m

m

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.8 0.9
0.6 0.7
0.4 0.5
0.3
0.1 0.2

1

Fig. 3. Meta-function of affective experience distribution.

Disin→m (SE ) = Disin→k (SE ) · Disik→m (SE ),

+ Σnj ∈{n1 ,n2 ,...,nt } (ωni j →m · EDnj (FE )).

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1

513

(14)

(SE ) = [min(SE ), max(SE )].

3.3. Affective experience distribution
Based on the above studies, we design an affective experience
distribution model to determine consistency. The model can analyze whether the participant hides their emotion or whether the
facial expression and affective experience are of inner consistency.
As the person-independent model can easily collect facial features
and effectively achieve facial expression recognition, facial features
and facial states are defined as affective indicator evidences. As
well as algorithms of synergetic dependence evaluation, the model

can analyze affective experience and represent a dimensional consistency distribution map. In such a manner, the distribution is
considered as the most individual intelligent method for facial expression analysis and affective computing.
Sections above describe the synergetic dependence evaluation
algorithm of the participant’s affective experience consistency
in an interactive environment. According to analysis of affective
evidences {SE , FE }, we design indicator dimensions for affective
experience and construct the consistency evaluation function in
this section.
In the process of models’ cooperative interaction, synergetic dependence evaluation of evidence is objective, and thus we apply FE
in Eq. (11) as the first dimension of affective experience distribution.
Since SE is an uncertain evaluation interval by calculating the
degree of coordination and distortion, we need a more detailed
quantitative value for SE’s synergetic interval as the second
dimension of affective experience distribution.
According to the model’s valuation interval, SyCom (SE ) for evidence SE in Eq. (14), the synergetic dependence value SCV m (SE ) is
designed as follows:
SCV m (SE ) = ωm,SDF · SDF m (SE ) + ωm,Dis · (1 − Dism (SE )),

(15)

where ωm,SDF and ωm,Dis represent the weighted values of the
model m’s coordination and distortion degree analysis to result in
SE with ωm,SDF + ωm,Dis = 1. The value SCV m (SE ) is the model m’s
synergetic dependence evaluation of SE through cooperative interaction with other models, and is suitable as the second dimension
of affective experience distribution.
Based on two dimensions of EDm (FE ) and SCV m (SE ), we design an affective experience distribution model. Within affective
indicator evidences {SE , FE }, we apply a synergetic dependence
evaluation of SE and FE to define f : (EDm , SCV m ) → DAE m as
the consistency function. First, the meta-type of the function is described as:
z = (1 − x)2 · e−a·|1−x| · |1 − y| · y2 · a3 , (x, y ∈ [0, 1]),

(16)

where x and y are two dimension parameters, and a is the positive regulatory factor. The meta-function can be shown in threedimensional space. From Fig. 3, we see that the x axis represents
the consistency evaluation of SE, the y axis represents the consistency evaluation of FE, and the z axis represents the affective experience distribution which can be used to determine consistency
of inner emotional status and external facial expressions.
In Fig. 3, the tendency of the x axis can be described as the
duration time of facial expression states, and it is calculated by
predicting the inference of facial expression states that represent
the person-independent model’s subjective evaluation according
to current conditions and motivations. The tendency of the y axis
can be described as the intensity of facial expression features,

514

C. Xu et al. / Future Generation Computer Systems 54 (2016) 507–517

and it represents the participant’s current expression in detail.
For example, exaggerated laughter and an ordinary smile are two
different intensities through facial feature expressions. Thus, the y
axis reflects the model’s objective evaluation to the participant’s
facial features. Based on subjective dimension x and objective
dimension y, it is suitable and trusted to apply the meta-function
z to describe affective experience consistency.
In meta-function (16) and Fig. 3, the participant’s affective
experience value increases and then decreases with the increase of
subjective evaluation of SE, and it is similar to objective evaluation
of FE. It is suggested that the participant may express hidden
emotional status if keeping some facial expression for a long time
or expressing exaggerated facial features.
As analyzed above, the meta-function (16) has a similar tendency with the participant’s affective experience consistency.
If personalized affective experience distribution parameters are
added into the meta-function, it can be used as the basic function
to analyze affective experience.
According to Eq. (16) and synergetic dependence evaluation of
SE and FE, we define AE’s consistency function f : (EDm , SCV m ) →
DAE m as:
FE
FE 2
DAE m (SE , FE ) = [EDm (FE ) · (1 − ωm
) − ωm
]
FE
FE
· |(1 + ωm
) − EDm (FE ) · (1 − ωm
)|
SE
FE 2
· [SCV m (SE ) · (1 + ωm
) − (1 + ωm
)]

· e−α·|SCV m (SE )·(1+ωm )−(1+ωm )| · α 3 ,
SE

FE

(17)

where DAE m (SE , FE ) is the model m’s consistency analysis of
AE through affective indicator evidences {SE , FE }; EDm (FE ) and
SCV m (SE ) are the model’s synergetic dependence evaluation to FE
FE
and SE based on Eqs. (11) and (15); the parameters of ωm
and
SE
ωm represent the weighted values of FE and SE; and a is the positive regulatory factor determined by the model’s experiences. It is
FE
noted that there is no direct constraint relations between ωm
and
SE
ωm , and they do not need to meet normalization conditions.
Form Eq. (17), it is clear that SE’s synergetic dependence evaluation is presented as the form of e−|SCV m (SE )| that reflects the continuity of facial expression state and can be explained as the affect of
time. For example, we can analyze whether the participant is good
at expressing inner emotional states by external facial expression,
or in which strength level of facial expression, the participant is
adept in hiding inner affective experience. On the other hand, FE’s
synergetic dependence evaluation is intensity information for facial features that is the indicator parameter to evaluate the participant’s facial expression (for example, laugh, smile, or laughter).
The person-independent model can construct the participant’s
affective experience distribution with Eq. (17) through cooperative
interaction in an interactive environment. The tendency of affective experience with FE and SE is the participant’s personalization,
and is also the model’s personalized emotional intelligence analysis ability.
Without doubt, consistency determination of affective experience does not provide totally accurate knowledge representation;
but in our model, cooperative interaction and synergetic dependence evaluation are proposed and applied to reduce the difficulty
of reasoning with uncertain knowledge. Therefore, the affective experience distribution model described above is available, reasonable and credible.
4. Experiment
Based on the analysis of affective indicator evidences and distribution above, we design an experiment to verify the proposed
model and approaches. The experimental platform is built on the
person-independent model within an interactive environment. We

achieve an analysis system of affective experience consistency
through an evaluation interface of affective indicator evidences in
Section 2, and a synergetic dependence evaluation algorithm in
Section 3. Finally, we simulate an interactive environment to calculate the participant’s affective experience distribution map, and
verify a personalized analysis model with promising accuracy.
4.1. Experiment design
The experiment system is extended from the personindependent model with cooperative interaction. A facial feature
cluster is applied to evaluate evidence FE, facial expression nets
are introduced to analyze the results of SE, and both {FE , SE } are
used to achieve reasoning as indicator dimensions of affective experience distribution.
There are four samples of a person-independent model presented in the experiments. They can interact with each other to
transfer affective indicator evidences. Each model is equipped with
a real-time camera with 300,000 pixels that can detect the participant online. With the help of cooperative interaction, models can
achieve the participant’s affective experience distribution maps in
three-dimensional space, seen in Fig. 4.
Affective evidence for FE and SE can be calculated by Eqs. (11)
and (15), yielding a synergetic dependence evaluation for affective experience analysis. According to affective indicator evidences
and synergetic dependence, the person-independent model can
build the participant’s affective experience distribution map with
Eq. (17). Meanwhile, the analysis model will determine the participant’s inner emotional status with current conditions and motivations in the distribution map, and then display the results by
logging into the system panel.
The experiment is limited to indoor scenes, and four participants are watching video and communicating with each other
occasionally. In order to verify the affective experience analysis
system, we determine the participants’ real-time emotional states
and analyze consistency between facial expression and affective
experience.
The purpose of the experiment is to find a participant who
can hide inner emotions. In other words, the analysis model tries
to determine whether there is distortion for the participant’s
emotions. The model can make intelligent judgments and friendly
interaction with the participant to achieve a better affective
experience by consistency determination and affective experience
distribution.
The experimental design and process is described in Table 1.
The experiment of affective experience above is the model’s
personalized evaluation of the participant’s inner emotional status.
The whole process is based on the models’ cooperative interaction
FE
SE
with evidences {SE , FE } including weighed values ωm
, ωm
and the
positive regulatory factor α .
4.2. Experiment result analysis
According to the experiment process above, participants A, B, C
and D’s affective experience distributions are presented in Fig. 4.
Fig. 4(a) shows participant A’s affective experience distribution in
which the x axis represents SE’s synergetic dependence evidence
evaluation SCV A (SE ), the y axis represents FE’s synergetic dependence evidence evaluation EDA (FE ), and the z axis is AE’s consistency evaluation DAE A (SE , FE ). The highest point in Fig. 4(a) is (1,
0.85, 0.80197). That means participant A expresses the largest consistency value of 0.80197 when synergetic dependence evaluations
of their facial expression state and facial feature are 1 and 0.85
respectively. Fig. 4(b)–(d) represents the related participants’ affective experience distribution maps, and we submit a general discussion next.

C. Xu et al. / Future Generation Computer Systems 54 (2016) 507–517

515

Table 1
The experimental design and process.
Input: The participant’s facial expression features and states, and other parameters that can be detected and collected.
Loop:
Step i: The person-independent model acquires the participant’s facial data, and then calculates facial features and facial expression state for affective indicator
evidences {SE , FE };
Step ii: Inside the model, it achieves affective evidences for FE and SE’s evaluations as EDm (FE ) and Com (SE ) according to Eqs. (2) and (5);
Step iii: Among models, they calculate synergetic dependence evaluation of EDm (FE ) as the first dimension and SCV m (SE ) as the second dimension for affective
distribution on the basis of multi-model cooperative interaction and Eqs. (11) and (15);
Step iv: Each model determines AE’s consistency value DAE m (SE , FE ) according to EDm (FE ) and SCV m (SE ) in Eq. (17), achieves AE’s constancy distribution
f : (EDm , SCV m ) → DAE m , and presents the map in three-dimensional space.
Output: Return the participant’s affective experience distribution map.

(a) Distribution of participant A.

(b) Distribution of participant B.

(c) Distribution of participant C.

(d) Distribution of participant D.
Fig. 4. Affective experience distribution samples.

Along the x axis, comparing the four sub-graphs in Fig. 4, the
consistency value DAE m (SE , FE ) of affective experience becomes
larger with the increase of SCV m (SE ). Between 0 < SCV m (SE ) <
0.5, the values DAE m (SE , FE ) are closely approach 0 for Fig. 4(a)–(c),
which means the affective experience of participants A, B and C is
difficult to determine by facial expression when their SE’s consistency is low or inadequate. However, with a further increase of SE’s
synergetic dependence evaluation, the affective experience of participants A, B and C begins to rise to maximum value. Participant
A has the largest value when SCV A (SE ) = 1; while participant B
and C’s affective experience appear slightly decreasing. That means
their true emotional status may be hidden when they express the
most exaggerated facial expressions with the values SCV B (SE ) = 1
and SCV C (SE ) = 1. Fig. 4(d) is different from the former three subgraphs, and participant D’s affective experience will continue to
decrease or increase after the largest value when SCV D (SE ) = 1.
It can be explained that participant D has complex emotional expressions and is good at affective presentation with facial expressions. In other words, participant D is used to hide inner emotions.
For example, participant D’s evaluation of SE is still very large even
when it has low AE so as to conceal anger or delight.

Along the direction of the y axis and comparing EDm (FE ), consistency values DAE m (SE , FE ) of affective experience all increase and
then decrease with an increase of EDm (FE ). Compared to SCV m (SE ),
EDm (FE ) is the supportive judgment factor for the participants objective external expression related to inner emotional status that
is measurable; while SCV m (SE ) reflects the characteristics of timeliness, the duration of the participant’s facial expression for inner
affective experience. Thus, SE is calculated from FE, and it contains
some affective information of FE.
Table 2 displays the results and the largest consistency of
DAE m (SE , FE ) by EDm (FE ) and SCV m (SE ) in Fig. 4. Facial Expression State is the value of SCV m (SE ); Facial Expression Feature
is the value of EDm (FE ); Affective Experience is the value of
DAE m (SE , FE ). From Table 2, when the person-independent model
shows the largest value of affective experience, model A has the
largest evaluation of SE as SCV A (SE ) = 1; model D has the largest
evaluation of FE as EDD (FE ) = 0.858. For AE, model D has the highest experience as DAE D (SE , FE ) = 0.812.
Fig. 5 presents bar chart of consistency evaluation for affective indicator evidences and affective experience distributions. It
is clear that all participants’ affective experience consistency has
similar peaks at about 0.8. That means the participants have similar

516

C. Xu et al. / Future Generation Computer Systems 54 (2016) 507–517

Table 2
Largest consistency distribution of affective experience.

achieve real-time analysis of affective experience and construct its
personalized distribution map in an interactive environment.

Synergetic dependence

Model A

Model B

Model C

Model D

SCV m (SE )
EDm (FE )
DAE m (SE , FE )

1.000
0.850
0.80197

0.900
0.775
0.80161

0.825
0.701
0.80077

0.750
0.858
0.81227

Fig. 5. Consistency evaluation for affective indicator evidences and affective
experience distributions.

Fig. 6. Sequence diagram and evaluation of affective experience consistency.

emotional expressions since they are all in the same interactive environment, and restricted by the same interactive norm
constraints. Meanwhile, there exist different evaluations to
the participants’ affective indicator evidences among personindependent models, and it is important to verify that each participant has their own emotional expression style, although they
are restricted by the same norm constraints of the interactive environment.
The models’ synergetic dependence evaluations are changing
with cooperative interaction when they analyze affective experience. We take Model B’s example and present the affective experience curve over time in Fig. 6.
Fig. 6 shows sample values of facial features, facial states and
affective experience. Since FE’s evaluation is objective, its value is
usually larger than the other two; while SE’s evaluation value is
often smaller than FE because it is the results of recognition and
predicting inference for FE; and AE is the smallest that need be calculated through FE and SE. With the passage of time, the personindependent model continuously adjusts its interactive strategy
and feedback to other models. Thus, cooperative interaction can
make models achieve better synergetic dependence which leads to
a gradually better participant affective experience and consistency
evaluation. It is argued that participant B does not hide inner emotions and there is good consistency between B’s facial expression
and affective experience.
Interactive norms constrain styles of affective presentation
by facial expressions. In an interactive environment, the personindependent model analyzes the participant’s facial expressions
and determines consistency of affective experience according to
the facial expression state and facial feature clusters within multimodels’ cooperative interaction. The analysis ability makes the
intelligent agent perform natural and vivid interaction similar to
human beings.
The experiments above simulate the models’ cooperative
interaction and verify the reasonableness and usability of the proposed model. The experiment results demonstrate that the personindependent model with synergetic dependence evaluation can

5. Conclusion
With the advances of the person-independent model with cooperative interaction, we propose and achieve a synergetic dependence evaluation model of affective experience consistency with
satisfactory results. Based on facial expression nets and predicting
inference, this paper presents the design and implementation of an
analysis model in an interactive environment with affective indicator evidences, and shows participant’ affective experience distribution map in three-dimensional space.
Additionally, experiments are conducted to evaluate the efficacy of the analysis model. The experiment results indicate that
affective experience analysis can be effective both in determining
a participant’s inner emotional status and in improving the rationality and collaboration of HCI by applying synergetic dependence
with cooperative interaction. The results of our study indicate that
human performance ability can be improved under certain given
tasks by mapping between facial expressions and affective experience, and utilizing machine vision. To the model design, our work
shows that synergetic dependence analysis can improve affective
experience determination performance, and affective indicator evidences based on an analytic hierarchy process and integrated reasoning are effective factors to improve the analysis model.
Our study has broader implications for affective computing research and sheds light on several potential factors that might affect
the efficiency of such a model design. The results of this current
research also suggest that applying synergetic dependence can
improve analysis performance in recognizing and comprehending
human emotions from facial expressions. An affective experience
distribution map is the most personalized emotional characteristic
feature for both participant and person-independent models proposed in this paper.
This paper also provides the basic significance for harmonious
affective computing. Thus, further experiments will be conducted
to study performance of affective recognition and synthesis.
Acknowledgments
The authors wish to thank Hui Wang, Chunhua Tao and Jia Hu.
This work is supported by the National Natural Science Foundation
of China (No. 61304262).
References
[1] G. Cockton, From doing to being: bringing emotion into interaction, Interact.
Comput. 14 (2) (2002) 89–92.
[2] C. Aruna, K. Amit, Emotional Intelligence: A Cybernetic Approach, Springer,
2009.
[3] Ryan S.J.d. Baker, Sidney K. D’Mello, Ma. Mercedes T. Rodrigo, Arthur C.
Graesser, Better to be frustrated than bored: the incidence, persistence, and
impact of learners’ cognitive–affective states during interactions with three
different computer-based learning environments, Int. J. Hum.–Comput. Stud.
68 (4) (2010) 223–241.
[4] R.W. Picard, Affective Computing, Massachusetts Institute of Technology Press,
London, 1997, pp. 1–83.
[5] J. Whitehill, M. Bartlett, J. Movellan, Automatic facial expression recognition
for intelligent tutoring systems, in: Proceedings of 2008 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition Workshops, AK, USA,
2008, pp. 1–6.
[6] D.L. Tennenhouse, Proactive computing, Commun. ACM 43 (5) (2000) 43–50.
[7] A. Oulasvirta, A. Salovaara, A cognitive meta-analysis of design approaches
to interruptions in intelligent environments, in: Proceedings of International Conference for Human–Computer Interaction, Vienna, Austria, 2004,
pp. 1155–1158.
[8] S. Leppinen, Research council for health academy of Finland, Research Programme on Health Services Research 2004–2007 Programme Memorandum.
http://www.aka.fi/. Research Council for Health Academy of Finland. 2007.

C. Xu et al. / Future Generation Computer Systems 54 (2016) 507–517
[9] K. Niemi, Research programme on environmental, societal and health effects
of genetically modified organisms 2004–2007 ESGEMO. http://www.aka.fi/.
Academy of Finland. 2007.
[10] L.C. Gerald, P. Janet, Affective guidance of intelligent agents: how emotion
controls cognition, Cogn. Syst. Res. 10 (1) (2009) 21–30.
[11] A. Chakraborty, A. Konar, U.K. Chakraborty, A. Chatterjee, Emotion recognition
from facial expressions and its control using fuzzy logic, IEEE Trans. Syst. Man
Cybern. A 39 (4) (2009) 726–743.
[12] M. Ghosh, A. Chakraborty, A. Acharya, et al. A recurrent neural model
for parameter estimation of mixed emotions from facial expressions of
the subjects, in: International Joint Conference on Neural Networks, 2009,
pp. 965–972.
[13] Z. Zeng, M. Pantic, G.I. Roisman, T.S. Huang, A survey of affect recognition
methods: audio, visual, and spontaneous expressions, IEEE Trans. Pattern Anal.
Mach. Intell. 31 (1) (2009) 39–58.
[14] R.W. Picard, Affective computing: challenges, Int. J. Hum.–Comput. Stud. 59
(1–2) (2003) 55–64.
[15] N. Tractinsky, Tools over solutions? Comments on Interacting with Computers
special issue on affective computing, Interact. Comput. 16 (4) (2004) 751–757.
[16] S. Brave, C. Nass, Emotion in human–computer interaction, in: J. Jacko, A. Sears
(Eds.), The Human–Computer Interaction Handbook, Lawrence Erlbaum
Associates, Mahwah, NJ, 2003.
[17] J.T. Cacioppo, W.L. Gardner, Emotion, Annu. Rev. Physiol. 50 (1999) 191–214.
[18] D.A. Norman, Emotion and design: attractive things work better, Interactions
(2002) 36–42.
[19] R.W. Picard, J. Klein, Computers that recognize and respond to user emotion:
theoretical and practical implications, Interact. Comput. 14 (2) (2002)
141–169.
[20] G. Lindgaard, Adventurers versus nit-pickers on affective computing, Interact.
Comput. 16 (4) (2004) 723–728. Interacting with Computers.
[21] J.T. Larsen, A.P. McGraw, J.T. Cacioppo, Can people feel happy and sad at the
same time? J. Pers. Soc. Psychol. 81 (4) (2001) 684–696.
[22] D. Kahneman, Experienced utility and objective happiness: a moment-based
approach, in: D. Kahneman, A. Tversky (Eds.), Choices, Values and Frames,
Cambridge University Press, New York, 2000.
[23] L.D.L. Vidrascu, Five emotion classes detection in real-world call center data:
the use of various types of paralinguistic features, in: Paraling 2007.
[24] P. Ekman, An argument for basic emotions, Cogn. Emot. 6 (3–4) (1992)
169–200.
[25] S.S. Tal, Automatic inference of complex affective states, Comput. Speech Lang.
25 (1) (2011) 45–62.
[26] R. Fernandez, R.W. Picard, Modeling drivers’ speech under stress, Speech
Commun. 40 (2003) 145–159.
[27] T.K. Kim, S. Kee, S.R. Kim, Real-time normalization and feature extraction of
3D face data using curvature characteristics, in: Proceedings of the 10th IEEE
International Workshop on Robot and Human Communication, Paris, France,
2001, pp. 74–79.
[28] L. Zalewski, S. Gong, Synthesis and recognition of facial expressions in virtual
3D views, in: Proceedings of IEEE 6th International Conference on Automatic
Face and Gesture Recognition, Seoul, Korea, 2004, pp. 493–498.
[29] J. Schreirer, R. Fernandez, J. Klein, R.W. Picard, Frustrating the user on purpose:
a step toward building an affective computer, Interact. Comput. 14 (2) (2002)
93–118.
[30] J. Klein, Y. Moon, R.W. Picard, This computer responds to user frustration:
theory, design, and results, Interact. Comput. 14 (2) (2002) 119–140.
[31] R.D. Ward, P.H. Marsden, Affective computing: problems, reactions and
intentions, Interact. Comput. 16 (4) (2004) 707–713.

517

[32] A. Whiten, Natural Theories of Mind, Basil Blackwell, Oxford, 1991.
[33] S. Baron-Cohen, in: M. Corballis, S. Lea (Eds.), The Descent of Mind:
Psychological Perspectives on Hominid Evolution, Oxford University Press,
1999.
[34] J.K. Hietanen, V. Surakka, I. Linnankoski, Facial electromyographic responses
to vocal affect expressions, Psychophysiology 35 (1998) 530–536.
[35] V. Surakka, J.K. Hietanen, Facial and emotional reactions to Duchenne and nonDuchenne smiles, Int. J. Psychophysiol. 29 (1998) 23–33.
[36] H. Haken, Synergetics—An Introduction, Springer, New York, 1977.
[37] N. Birbaumer, A. Öhman, The Structure of Emotion: Psychophysiological,
Cognitive and Clinical Aspects, Hogrefe and Huber Publishers, Seattle, 1993.
[38] G.Q. Xu, Z.Y. Feng, H.B. Wu, et al., Swift trust in a virtual temporary system:
a model based on the Dempster–Shafer theory of belief functions, Electron.
Commer. 12 (2007) 93–126.
[39] L.J. Savage, The Foundations of Statistics, Wiley, New York, 1954.
[40] J.M. Keynes, A Treatise on Probability, Macmillan and Co., London, 1921.
[41] D. Meyerson, K.E. Weick, R.M. Kramer, Swift Trust & Temporary System,
Stanford University, Trust in Organization, America, 1994, pp. 221–264.

Chao Xu is an associate professor at the School of Computer Software, Tianjin University, where he received his
Ph.D. His research interests include affective computing,
pattern recognition, proactive computing, HCI and knowledge management. He is a member of China Computer
Federation, IEEE and ACM.

Zhiyong Feng is a professor and associate director
of the School of Computer Science and Technology,
Tianjin University. His research interests lie in Affective
Computing, Cloud Computing, HCI, pervasive computing
and information security. He is a member of the IEEE and
a member of the board of supervisors of China Computer
Federation.

Zhaopeng Meng is a professor and director of the School
of Computer Software, Tianjin University. His research
interests lie in Collaborative systems, Big Data and Could
Computing.

